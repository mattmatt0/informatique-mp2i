\newcommand\PATH{Lancez `make adapt avant de compiler!`}
\RequirePackage{fix-cm}
\documentclass{scrartcl}
\input{\PATH/../include/preamble.tex}
\input{\PATH/../include/math-tools.tex}
\input{\PATH/../include/it-tools.tex}
\usepackage{bbm}
\usepackage{lmodern}



\renewcommand{\cal}[1]{\mathcal{#1}}
\newcommand{\scr}[1]{\mathscr{#1}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\bbm}[1]{\mathbbm{#1}}
\renewcommand{\rm}[1]{\mathrm{#1}}

\newcommand{\elt}[1]{\textbf{#1}}
\newcommand{\dotp}{\! \cdot \!}
\newcommand{\x}{\! \times \!}

\newcommand{\nl}{\\[5mm]}

\newcommand{\1}{\bbm{1}}
\newcommand{\Bij}{\scr{B} \hspace{-0.5mm} i \! j}

\title{Schémas Algorithmiques}
\author{}
\date{}

\begin{document}
	\maketitle
	\begin{center}
		\textsl{Dans ce chapitre, on présente différents paradigmes algorithmiques qui permettent de développer des algorithmes résolvant des problèmes (souvent d'optimisation) mettant en jeu des textes, des nombres, des listes, des graphes, des expressions arithmétiques, des points du plan, des ordonnancements...}
	\end{center}
	
	\plabel{Rappel :} On rappelle qu'un problème d'optimisation consiste à minimiser une fonction à valeurs réelles données sur un ensemble de solutions donné.

	\definition Ainsi, un tel problème $\mathcal{P}$ est défini par la fonction dite fonction-objectif $f$ et l'ensemble des solutions $\mathcal{S}$. \\
	On note 
	$\mathcal{P} : \max_{X \in \, \mathcal{S}} f(X) $ 
	(ou bien $ \mathcal{P} : \min_{X \in \, \mathcal{S}} f(X) $ s'il s'agit de minimiser $f$). \\
	On appelle alors valeur du problème $\mathcal{P}$ :
	$\mathrm{val} (\mathcal{P}) = \max \, \! \left\{ f(X) \ \middle| \, X \in \mathcal{S} \right\} $. \\
	On note aussi $\displaystyle \underset{X \in \, \mathcal{S}}{\mathrm{argmax}} \: f(X) = \set{X \in \mathcal{S} \ \tq \, f(X) = \mathrm{val} (\mathcal{P})}$ l'ensemble des solutions optimales.

	\rem L'unicité de la valeur optimale n'entraîne pas celle des solutions optimales.

	\rem Si \( \mathcal{S} \subseteq \widetilde{\mathcal{S}} \subseteq \mathcal{D} (f)\), on dit que \( \widetilde{\mathcal{P}} = \displaystyle \max_{X \in \, \mathcal{S}} f(X) \) est un relâché de $\mathcal{P}$. 
	On a alors \( \mathrm{val} (\widetilde{\mathcal{P}}) \geq \mathrm{val} (\widetilde{\mathcal{P}}) \).
	
	\section{Algorithmes gloutons}
	
		\subsection{L'exemple du problème du sac à dos}
		
			\[\pb{Sac à dos}{SUS}{AMOGUS}\]	
			\textsf{\textbf{\underline{SAC À DOS}}} \hspace{-0.6mm}
			\begin{tabular}[t]{|| l }
				\underline{entrée :}
					\begin{tabular}[t]{l}
						$P \in \mathbb{R}$ \\
						$v = (v_i)_{i \in [1 .. n]} \in (\mathbb{R^{+*}})^n$ \\
						$p = (p_i)_{i \in [1 .. n]} \in (\mathbb{R}^{+*})^n$ \\
					\end{tabular}
				\\
				\underline{sortie :} \( \, \underset{\substack{\delta \in \{ 0,1 \}^n \\ \delta \cdot p \leq P}}{\max} \delta \dotp v = \!\! \underset{\substack{\delta \in \{ 0,1 \}^n \\ \sum_{i=1}^{n} \delta_i p_i \leq P}}{\max} \displaystyle \sum_{i=1}^{n} \delta_i v_i \)
			\end{tabular}
			
			\vspace{2mm}
			\textsf{\textbf{\underline{SAC À DOS FRACTIONNAIRE}}} \hspace{-0.6mm}
			\begin{tabular}[t]{|| l }
				\underline{entrée :}
				\begin{tabular}[t]{l}
					$P \in \mathbb{R}$ \\
					$v = (v_i)_{i \in [1 .. n]} \in (\mathbb{R^{+*}})^n$ \\
					$p = (p_i)_{i \in [1 .. n]} \in (\mathbb{R}^{+*})^n$ \\
				\end{tabular}
				\\
				\underline{sortie :} \hspace{1mm} \( \underset{\substack{\delta \in [0,1]^n (*) \\ \delta \cdot p \leq P}}{\max} \delta \! \cdot \! v \)
			\end{tabular}
		
			\rem \textsf{\textit{Sac à dos fractionnaire}} est un relâché de \textsf{\textit{Sac à dos}}.
				%
				\\[5mm]
				%
			\rem La contrainte $(*)$ s'écrit aussi/se traduit aussi par : \hspace{-4.2mm}
				\begin{tabular}{l}
					\( \forall \, i \in [1..n], \, \delta \! \cdot \! \mathbbm{1}_{\{i\}} \geq 0 \) \\
					\( \forall \, i \in [1..n], \, \delta \! \cdot \! \mathbbm{1}_{\{i\}} \leq 1 \). \\
				\end{tabular}
				\\[-2mm]
			En effet, \( \forall \, i \in [1..n], \, \delta \dotp \1_{i} = \displaystyle \sum_{j=1}^{n} \delta_j \x \hspace{-5mm} \underbrace{(\1_i)_j}_{=0 \text{ sauf si } j = i} \hspace{-5mm} = \delta_i \x (\1_i)_i = \delta_i \x 1 = \delta_i \).
				\nl
				%
			\elt{Exemple :} Pour $P = 20$, en choisissant par $v_i$ décroissants, on trouve \( \delta^* = \begin{psmallmatrix} 1 \\ 0 \\ 0 \end{psmallmatrix} \).
			\\[-3mm]
				\begin{tabular}[t]{ll}
					\begin{tabular}[t]{| c | c | c |}
						\hline
						$i$ & $p_i$ & $c_i$ \\ \hline
						1 & 20 & 10 \\ \hline
						2 & 10 & 9 \\ \hline
						3 & 10 & 9 \\ \hline
					\end{tabular}
					&
					\parbox[t]{120mm}{
						\vspace{1mm}
						Alors, $\delta^* \! \dotp v = v_1 = 10$. \\
						Or, \( \delta = \begin{psmallmatrix} 0 \\ 1 \\ 1 \end{psmallmatrix} \) vérifie
						\hspace{-8mm}
							\begin{tabular}[t]{l}
								\hspace{3.6mm} \( \delta \dotp p = 1 \x p_2 + 1 \x p_3 = 10 + 10 = 20 \leq P \) \\
								et \( \delta \dotp v = 1 \x v_2 + 1 \x v_3 = 9 + 9 = 18 > 10 \).
							\end{tabular} \\
						Ainsi, cette stratégie n'est pas optimale.
					}
				\end{tabular}
				\nl
				%
			\elt{Exemple :} Toujours pour $P = 20$, avec des $p_i$ croissants, on a \( \delta^* = \begin{psmallmatrix} 0 \\ 1 \\ 1 \end{psmallmatrix} \), de valeur $\delta^* \! \dotp v = 2$.
			\\[-3mm]
				\begin{tabular}[t]{ll}
					\begin{tabular}[t]{| c | c | c |}
						\hline
						$i$ & $p_i$ & $c_i$ \\ \hline
						1 & 18 & 10 \\ \hline
						2 & 10 & 9 \\ \hline
						3 & 10 & 9 \\ \hline
					\end{tabular}
					&
					\parbox[t]{140mm}{
						\vspace{1mm}
						Or, \( \delta = \begin{psmallmatrix} 1 \\ 0 \\ 0 \end{psmallmatrix} \) est une solution, car \( \delta \dotp p = 18 \leq P \), et est de valeur \( \delta \dotp v = 10 > 2 \).
							\\[2mm]
						On en déduit donc de même que cette stratégie n'est pas optimale.
						\hspace{-8mm}
					}
				\end{tabular}
				\nl
				%
			\elt{Exemple :} Si on sélectionne maintenant par $(v_i / p_i)$ décroissants, on obtient $\delta^* \! \dotp v = v_1 = 22$.
			\\[-1mm]
			\begin{tabular}[t]{ll}
				\begin{tabular}[t]{| c | c | c | c |}
					\hline
					$i$ & $p_i$ & $c_i$ & $v_i/p_i$\\ \hline
					1 & 11 & 22 & 2\\ \hline
					2 & 10 & 15 & 1,5 \\ \hline
					3 & 10 & 15 & 1,5 \\ \hline
				\end{tabular}
				&
				\parbox[t]{130mm}{
					\vspace{1mm}
					Or, \( \delta = \begin{psmallmatrix} 0 \\ 1 \\ 1 \end{psmallmatrix} \) est une autre solution de valeur meilleure :
						\vspace{-5mm}
						\[ \delta \dotp v = 15 + 15 = 30 > 22 \]
						
					\vspace{-3mm}
					Ainsi, cette stratégie n'est pas optimale non plus.
				}
			\end{tabular}
				\nl
			\rem Les exemples précédents montrent que pour le problème \textsf{\textit{Sac à dos}}, les stratégies simples consistant à prendre les objets simplement par $v_i$ décroissants, $p_i$ croissants ou $v_i/p_i$ décroissants ne sont pas optimales. En fait, \textsf{\textit{Sac à dos}} est un problème difficile $^{\text{(?)}}$.
				\\[2mm]
			Par contre, le dernier des trois exemples précédents s'avèrera utile pour le problème du \textsf{\textit{Sac à dos fractionnaire}}.
				\nl
				%
			\elt{Exemple :} Sur le dernier exemple, \( \delta^* = \left( \!\! \begin{smallmatrix} 1 \\ 9/10 \\ 0 \end{smallmatrix} \!\! \right) \) convient en tant que solution optimale, \\[-2mm] et \( \delta^* \! \dotp v = 22 + \text{13,5} = \text{35,5} \).
				\nl
				%
			\renewcommand{\arraystretch}{0.7}
			\elt{\underline{$\boldsymbol{*}$ Algorithme en pseudo-code}} \\[2mm]
				\begin{tabular}{l | l}
					\hspace{-3.5mm} \underline{\textsf{Rempli-sac}} 
					&
					\hspace{-3.8mm} \begin{tabular}[t]{l}
						$P$ un réel \\
						\( (p_i, v_i)_{i \in [1..n]} \in \left( \! (\bb{R}^{+*})^2 \right) ^n \) une suite finie de réels strictement positifs.
					\end{tabular}
				\end{tabular}
			\renewcommand{\arraystretch}{1}
				\\[3mm]
				%
				\hspace*{10mm}
				\begin{tabular}{|| l}
						\parbox{150mm}{
							Trier les objets par ratio $v_i / p_i$ décroissant, \emph{i.e.} trouver \( \sigma \in \Bij ([1..n], [1..n]) \) tel que \( \displaystyle \left( v_{\sigma(k)} / p_{\sigma(k)} \right)_{k \in [1..n]} \) soit décroissante.
							%\bigg( \! \frac{v_{\sigma(k)}}{p_{\sigma(k)}} \! \bigg)_{k \in [1..n]}
							\\
							$s \leftarrow 0$
							\\
							$\delta \leftarrow$ tableau de réels indicé par $[1..n]$ initialisé à 0
							\\
							$k = 1$
							\begin{center}[M]{Tant que $k \leq n$ et $s + p_{\sigma(k)} \leq P$ \hspace*{4mm} $/ \hspace{-1mm} /$ \textit{Invariant} : \( \textstyle s = \sum_{i=1}^{n} \delta_i p_i \)}
								\( s \leftarrow s + p_{\sigma(k)} \) \\
								\( \delta_{\sigma(k)} \leftarrow 1 \) \\
								\( k \leftarrow k + 1 \)
							\end{center}
						}
				\end{tabular}
				\\
				\hspace*{10mm}
				\begin{tabular}{|| l}
					\parbox{150mm}{
						\begin{center}[M]{Si $k \leq n$}
							\( \delta_{\sigma(k)} \leftarrow (P - s)/p_{\sigma(k)} \) \\
							\( s \leftarrow s + \delta_{\sigma(k)} p_{\sigma(k)} \)
						\end{center}
						Renvoyer $\delta$
					}
				\end{tabular}
			
			On montre l'optimalité de cette stratégie par un argument d'échange.
				\\[0mm]
				%
			\begin{center}[S]{\elt{\underline{Propriété :}}}
				Soit $n \in \bb{N}^*$. \\
				Soit \( (p_i, v_i)_{i \in [1..n]} \in (\bb{R}^{+*} \x \bb{R}^{+*})^2 \) telle que \( \left( v_i/p_i \right)_{i \in [1..n]} \) soit strictement décroissante. \\[-0.8mm]
				Soit $P \in \bb{R}^{+*}$. \\
				On note \( \cal{S} = \left\{ \delta \in [0, 1]^n \, \middle| \, \delta \dotp p \leq P \right\} \).
				\\[1mm]
				$\trianglerighteq$ Si \( p \dotp \1 > P \) (\emph{i.e.} \( \displaystyle \sum_{i=1}^n p_i > P \) ou encore \( \1 \notin \cal{S} \)), alors si \( \delta^* \in \underset{\delta \in \cal{S}}{\mathrm{argmax}} \: v \dotp \delta \), il existe \( m \in [1..n] \) \\[-3.2mm] tel que :
				\\[-8mm] \renewcommand{\arraystretch}{1.2}
					\[
						\left\lbrace \hspace{-1mm}
						\begin{tabular}{l}
							\( \forall \, i \in [1..m[, \, \delta^*_i = 1 \) \\
							\( \delta^*_m = \displaystyle \frac{P - \sum_{k=1}^{m-1} p_k}{p_m} \) \\
							\( \forall \, i \in [m+1..n], \delta^*_i = 0 \)
						\end{tabular} \right.
					\]
				\renewcommand{\arraystretch}{1}
			\end{center}
				%
			\elt{Preuve :} Soit \( \delta^* \in \underset{\delta \in \cal{S}}{\mathrm{argmax}} \: v \dotp \delta \). \\
			$\triangleright$ On sait que $\1 \notin \cal{S}$, donc $\delta^* \neq \1$ et donc \( \left\{ i \in [1..n] \, \middle| \, \delta^*_i < 1 \right\} \neq \emptyset \).
			On peut alors définir \( m = \min \, \{ i \in [1..n] \, | \, \delta^*_i < 1 \} \).
			Par définition de $m$, \( \forall \, i \in [1..m[, \, \delta^*_i = 1 \).
				\\[3mm]
			$\triangleright$ Par l'absurde, on suppose qu'il existe \( i_0 \in [m+1..n] \) tel que \( \delta^*_{i_0} \neq 0 \). \\
			Posons \( \varepsilon = \min \big( \!\!\!\!\! \underbrace{p_{i_0} \delta^*_{i_0}}_{> 0 \text{ par hyp.}} \!\!\!\! , \, \underbrace{p_m (1 - \delta^*m)}_{> 0 \text{ car } \delta^*_m < 1} \hspace{-0.6mm} \big) \).
			On considère alors \( \widehat{\delta} = \delta^* - \displaystyle \frac{\varepsilon}{p_{i_0}} \1_{i_0} + \frac{\varepsilon}{p_m} \1_{i_0}\).
				\\
			Soit $i \in [1..n]$. \\
			\hspace*{5mm} \parbox{170mm}{
				$\bullet$ Si \( i \notin \{ i_0, m \} \), \( \widehat{\delta}_i = \delta^*_i \in [0, 1] \). \\[1mm]
				$\bullet$ \( \displaystyle \widehat{\delta}_{i_0} = \delta^*_{i_0} - \frac{\varepsilon}{p_{i_0}} \leq \delta_{i_0} \leq 1 \). \\[-1mm]
				De plus, \( \displaystyle \frac{\varepsilon}{p_{i_0}} \leq \frac{1}{p_{i_0}} \times p_{i_0} \delta^*_{i_0} = \delta^*_{i_0}\) donc \( \displaystyle \delta^*_{i_0} - \frac{\varepsilon}{p_{i_0}} \geq 0 \) soit \( \widehat{\delta}_{i_0} \geq 0 \), donc \( \displaystyle \delta^*_{i_0} - \frac{\varepsilon}{p_{i_0}} \geq 0 \). \\[1mm]
				$\bullet$ \(\displaystyle \widehat{\delta}_m = \delta^*_m + \frac{\varepsilon}{p_m} \geq \delta^*_m \geq 0 \). %\\[-1mm]
				De plus, \( \displaystyle \frac{\varepsilon}{p_m} \leq 1 - \delta^*_m \), soit \( \displaystyle \delta^*_m + \frac{\varepsilon}{p_m} \leq 1 \) soit \( \widehat{\delta}_m \leq 1 \).
			}
			Ainsi, \( \widehat{\delta} \in [0, 1]^n \).
				\\[2.5mm]
				%
			\hspace*{-3.5mm}
				\begin{tabular}[t]{ll}
					De plus, \hspace{-4mm} &
					\begin{tabular}[t]{ll}
						\( \widehat{\delta} \dotp p \) \hspace{-4mm}
						& \( = \displaystyle \left( \! \delta^* + \frac{\varepsilon}{p_m} \1_m - \frac{\varepsilon}{p_{i_0}} \1_{i_0} \! \right) \! \dotp p \) \\
						& \( = \displaystyle (\delta^* \dotp p ) + \frac{\varepsilon}{p_m} \underbrace{(\1_m \dotp p)}_{= p_m} - \frac{\varepsilon}{p_{i_0}} \underbrace{(\1_{i_0} \dotp p)}_{= p_{i_0}} \) \\[-2.5mm]
						& \( = \delta^* \dotp p + \cancel{\varepsilon} - \cancel{\varepsilon} \) \\
						& \( = \delta^* \dotp p \leq P \) \quad (car $\delta^* \in \cal{S}$)
					\end{tabular}
				\end{tabular} \\
			On a donc bien $\widehat{\delta} \in \cal{S}$.
				\\[3.5mm]
				%
			\hspace*{-3.5mm}
				\begin{tabular}[t]{ll}
					Ensuite, \hspace{-4mm} &
					\begin{tabular}[t]{ll}
						\( \widehat{\delta} \dotp v \) \hspace{-4mm}
						& \( = \displaystyle \delta^* \dotp v + \frac{\varepsilon}{p_m} \underbrace{(\1_m \dotp v)}_{=v_m} - \frac{\varepsilon}{p_{i_0}} \underbrace{(\1_{i_0} \dotp v)}_{=v_{i_0}} \) \\
						& \( = \displaystyle \delta^* \dotp v + \underbrace{\varepsilon}_{> 0 \, ^{(1)}} \Big( \! \underbrace{\frac{v_m}{p_m} - \frac{v_{i_0}}{p_{i_0}}}_{> 0 \, ^{(2)}} \! \Big) \) \\[-4mm]
						& \( > \delta^* \dotp v \)
					\end{tabular}
				\end{tabular} \\
			C'est absurde car \( \delta^* \in \underset{\delta \in \, \cal{S}}{\rm{argmax}} \: v \dotp \delta \).
			Donc \( \forall \, i \in [1..m], \, \delta^*_i = 0 \).
			%
		\pagebreak
		\begin{center}[S]{\elt{\underline{Corollaire :}}}
			Sous les hypothèses et notations de la propriété précédente, \( \underset{\delta \in \cal{S}}{\rm{argmax}} \: v \dotp \delta \) est réduite à la \\[-2.5mm] solution donnée par :
				\vspace{-5mm}
				\[
					M = \min \left\{ i \in [1..n] \, \middle| \, \sum_{k=1}^{i} p_k > P \right\} \!, \hspace{3mm} \text{\emph{c-à-d}} \hspace{3mm} ^t \big(\underset{\underset{\scriptstyle{M-1}}{\textstyle \xleftrightarrow{\hspace*{13mm}}}}{1 \ \, 1 \ \, ... \ \, 1} \ \, w \ \, \underset{\underset{\scriptstyle n-M}{\xleftrightarrow{\hspace*{9mm}}}}{0 \ \, ... \ \, 0} \big)
				\]
				\vspace{-4mm}
		\end{center}
		\elt{Preuve :} Soit \( \delta^* \in \argmax{\delta \in \cal{S}} \delta \dotp v \). \\
		Soit \( m = \min \left\{ i \in [1..n] \, \middle| \, \delta^*_i < 1 \right\}\). On veut montrer que \( m = M \). \\
		\hspace*{-3.6mm}
		\begin{tabular}{ll}
			Puisque \( \delta^* \in \cal{S} \), \( \delta^* \dotp p \leq P \)
			\hspace{-4mm} & soit \( \displaystyle \sum_{k=1}^n \delta^*_k p_k = \sum_{k=1}^{m} \delta^*_k p_k \leq P \) \\
			\hspace{-4mm} & soit \( \displaystyle \underbrace{\delta^*_m p_m}_{\geq 0} + \sum_{k=1}^{m-1} \delta^*_k p_k \leq P \) \\
			\hspace{-4mm} & donc \( \displaystyle \sum_{k=1}^{m-1} \underbrace{\delta^*_k}_{=1} p_k \leq P \) \\
			\hspace{-4mm} & donc \( m - 1 < M \) soit \( m \leq M \).
		\end{tabular} \\[1mm]
			%
		De plus, on a montré que \( \displaystyle \sum_{k=1}^{m} p_k > p \) (\emph{cf.} \( (*) \) dans la preuve précédente), \\[-1mm] donc \( m \in \left\{ i \in [1..n] \, \middle| \, \sum_{k=1}^i p_k > P \right\} \), donc \( m \geq M \) qui est le minimum de cet ensemble.
		
	\subsection{Le problème du tri}
		
		\textsf{\textbf{\underline{TRI}}} \hspace{-0.6mm}
			\begin{tabular}[t]{|| l }
				\underline{entrée :} \( \big( x_i \big)_{i \in [1..n]} \in X^n \), où \( (X, \leq) \) est un ensemble totalement ordonné.
				\\
				\underline{sortie :} \( \sigma \in \scr{S}_n \) telle que \( \big( x_{\sigma(i)} \big)_{i \in [1..n]} \) soit croissante pour \( \leq \).
			\end{tabular}
			\nl
		\rem \textsf{\textit{Tri}} est un problème d'optimisation, il consiste en la recherche de :
			\vspace{-3.8mm}
			\[
				\textstyle \underset{\sigma \in \scr{S}_n}{\min} \left( \sum_{i=1}^{n-1} \max (x_{\sigma(i)} - x_{\sigma(i+1)}, 0) \! \right) \! ,
			\]
			
			\vspace{-6.7mm}
		minimum qui vaut 0 et qui est atteint par n'importe quel \( \sigma \in \scr{S}_n \) solution du problème.
			\nl
			%
		\elt{\underline{$\boldsymbol{*}$ Algorithme en pseudo-code pour le tri par sélection}} \\[2mm]
			\begin{tabular}{l | l}
				\hspace{-3.5mm} \underline{\textsf{Tri-sélection}} 
				& 
				\hspace{-3.8mm} \begin{tabular}[t]{l}
					\( \left( x_i \right)_{i \in [1..n]} \) une famille d'éléments comparables avec \( \leq \) en \( \varTheta (1) \), rangés \\ dans un tableau.
				\end{tabular}
			\end{tabular}
		\\[3mm]
		%
		\hspace*{10mm}
		\begin{tabular}{|| l}
			\parbox{150mm}{
				Soit \( T \) une copie du tableau (\emph{i.e.} \( \forall \, i \in [1..n], T[\, i \,] = x_i \)) \\
				Soit \( I \) un tableau identité de \( [1..n] \) \\
				Soit \( \sigma \) un tableau d'entiers indicé par \( [1..n] \) initialisé à 0 \\
				\begin{center}[M]{Pour $k$ allant de 1 à $n$}
					\hspace*{4mm} $/ \hspace{-1mm} /$ \textit{Invariant} : \( \textstyle \forall i \in [1..n], \, T[\, i \,] = x_{I[\, i \,]} \) \\
					\hspace*{31.5mm} \( T[1..k-1] \) est trié \\
					\hspace*{31.5mm} \( \forall i \in [k..n], \, T[\, i \,] \geq \max \{ T[\, j \,] \, | \, j \in [1..k-1] \} \) \\
					\hspace*{31.5mm} \( \forall \, i \in [1..k-1], \, T[\, i \,] = x_{\sigma[\, i \,]}\)
					 	\\[1mm]
					Trouver \( i_0 \in \argmin{i \in [k..n]} T[\, i \,] \) \\
					\( \sigma[\, k \,] \leftarrow I[\, i_0 \,] \) \\
					Échanger \( T[\, k \,] \) et \( T[\, i_0 \,] \) \\
					Échanger \( I[\, k \,] \) et \( I[\, i_0 \,] \)
				\end{center}
				Renvoyer $\sigma$
			}
		\end{tabular}
			\nl
			%
		\rem Cet algorithme peut être optimisé : en effet, si on avait directement initialisé $\sigma$ au tableau identité, on n'aurait pas eu besoin du tableau $I$.
			\\[-0mm]
			%
		\begin{center}[S]{\underline{\elt{Propriété :}}}
			Soit $I$ un ensemble fini non  vide de cardinal $n$ (typiquement $[1..n]$ ou $[0..n-1]$). \\
			Soit $(x_i)_{i \in I} \in X^I$ où \( (X, \leq) \) est un ensemble totalement ordonné. \\
			Soit \( i_1 \in \argmin{i \in I} x_i \). \\
			On note \( \widetilde{I} = I \backslash \{i_1\} \).
				\\[1mm]
			Si \( \widetilde{\sigma} \in \Bij \big( [1..n-1], \widetilde{I} \, \big) \) est telle que \( \left( x_{\widetilde{\sigma}(i)} \right)_{i \in [1..n-1]} \) est croissante, alors le prolongement \( \sigma \) de \( \widetilde{\sigma} \) défini suivant est une bijection telle que \( \left( x_{\sigma(i)} \right)_{i \in [1..n]} \) est croissante :
				\vspace{-4mm}
				\setlength{\tabcolsep}{2.2pt}
					\[ 
						\sigma = \hspace{-0.2mm} \left( \! \begin{tabular}{rcl}
							\( [1..n] \) & \( \rightarrow \) & \( I \) \\
							1 & \( \mapsto \) & \( i_1 \) \\
							\( i \geq 2 \) & \( \mapsto \) & \( \widetilde{\sigma} (i-1) \)
						\end{tabular} \! \right)
					\]
					
				\vspace{-5mm}
		\end{center}
			%
		\elt{Preuve :} \emph{cf.} annexe ``Tri par sélection''.
		
	\subsection{À retenir sur les algorithmes gloutons}
		
		\vspace{2mm}
		\begin{center}[S]{\elt{\underline{Définition :}}}
			On dit qu'un algorithme qu'il est glouton lorsqu'il construit une solution à un problème (d'optimisation) en prenant des décisions localement pertinentes (\emph{c-à-d} optimales) à chaque étape et qu'il ne revient par sur ces décisions.
				\\[3mm]
			Dans le cadre d'un problème d'optimisation, on dit qu'un algorithme glouton est optimal s'il renvoie toujours une solution optimale.
		\end{center}
	
		\elt{Exemples :} L'algorithme de Huffman (\emph{cf.} DM n°3 - Partie 2/2) et l'algorithme de résolution du problème du \textsf{\textit{Sac à dos fractionnaire}} sont des algorithmes gloutons.
			%
			\nl
		\rem Attention, selon les problèmes, un même algorithme peut être optimal ou non. En général, les algorithmes gloutons sont efficaces (faible complexité, a fortiori polynomiale) : ils ne sont donc pas exacts pour les problèmes difficiles/NP-complets (comme \textsf{\textit{Sac à dos entier}}).
			\\[2mm]
		Néanmoins, ils peuvent être utiles pour obtenir rapidement la borne supérieure ou inférieure, \emph{c-à-d} un majorant ou un miniorant de la valeur optimale.
			%
			\nl
		\rem Pour savoir si un problème d'optimisation peut être résolu par un algorithme glouton, on se pose deux questions : \\
			\hspace*{4mm}
		\parbox[t]{164mm}{
			$\bullet$ La fonction-objectif est-elle décomposable comme somme de fonctions sur les sous-parties de la solution ? \\
			$\bullet$ L'ensemble des solutions est-il décomposable comme produit cartésien ou au contraire défini par des contraintes liantes ?
		}
	
\section{Programmation dynamique}

	\subsection{Le problème du sac à dos entier}
		
		Voir TP n°13 - Introduction à la programmation dynamique.
		
	\subsection{Plus long sous-mot commun}
	
		\begin{center}
			\textit{Dans cette section, on fixe $\Sigma$ un alphabet (\emph{i.e} un ensemble fini non vide de symboles).}
		\end{center}
			%
			\vspace{2mm}
		\begin{center}[S]{\elt{\underline{Rappel :}}}
			Soit \( (u, v) \in (\Sigma^*)^2 \). Notons \( n = |u| \) et \( m = |v| \).
				\\[1.5mm]
			On dit que $u$ est un sous-mot de $v$ et on note \( u \preccurlyeq v \) ssi il existe \( \varphi \in \scr{F} ([1..n], [1..m]) \) strictement croissante, telle que : \quad
				\( u = v_{\varphi(1)} v_{\varphi(2)} ... v_{\varphi(n)} \), \quad \emph{c-à-d} \quad \( \forall \, i \in [1..n], \, u_i = v_{\varphi(i)} \).
				\vspace{-1mm}
		\end{center}
			%
			\vspace{1mm}
		\textsf{\textbf{\underline{PLSMC}}} \hspace{-0.6mm}
		\begin{tabular}[t]{|| l }
			\underline{entrée :} \( (u, v) \in (\Sigma^*)^2 \)
			\\
			\underline{sortie :} \hspace{1mm} \( \max \left\{ |w| \, \middle| \, w \preccurlyeq u \text{ et } w \preccurlyeq v \right\} \) ou \( w^* \in \text{argmax} \left\{ |w| \, \middle| \, w \preccurlyeq u \text{ et } w \preccurlyeq v \right\} \)
		\end{tabular}
			%
			\nl
			\renewcommand{\arraystretch}{0.8}
		\begin{center}[S]{\elt{\underline{Propriété :}}}
			Soit \( (u, v) \in (\Sigma^*)^2 \). Notons \( n = |u| \) et \( m = |v| \).
				\\[1.5mm]
			Pour \( (i,j) \in [0..n] \times [0..m] \), on note
				\vspace{-4mm}
				\[
					\cal{L}_{i,j} = \max \left\{ |w| \, \middle| \, w \in \Sigma^*, \hspace*{-1mm}
						\begin{tabular}{l}
							$\scriptsize w \preccurlyeq u_1 ... u_i$ \\
							$\scriptsize w \preccurlyeq v_1 ... v_j$
						\end{tabular} \hspace{-2mm}
					\right\}
					= \underset{w \in \cal{S}_{i,j}}{\max} \: |w|
				\]
			\renewcommand{\arraystretch}{1}
			\begin{tabular}[t]{ll}
				On a alors : \hspace*{-6.5mm} &
				\begin{tabular}[t]{l}
						$\bullet$ \( \forall \, i \in [0..n], \, \cal{L}_{i,0} = |\varepsilon| = 0 \) \\
						$\bullet$ \( \forall \, j \in [0..m], \, \cal{L}_{0,j} = 0 \) \\
						$\bullet$ \( \forall (i, j) \in [0..n] \times [0..m], \displaystyle \cal{L}_{i,j} = \lbrace \)
				\end{tabular}
			\end{tabular}
		\end{center}
\end{document}
